Så i starten tok jeg kontakt med fagskolen for å se hva jeg kunne gjøre, om vi kunne fått et samarbeid her. Det var det veldig positivt til, og vi begynte å snakke om de forskjellige mulighetene vi hadde. Dette var i oktober. Da landet vi på enten å lage noe for Mari-teamavdeling, eller veiledning av lærlinger. Jeg endte opp med å velge veiledning av lærlinger for å kunne ha den samtale-AI'en som jeg har utviklet i dag. De ønsket å kunne gi studentene mer mengdetrening. De ønsket også å kunne gi nettstudentene noe de faktisk kunne benytte seg av i tillegg for denne mengdetreningen. Per dag hadde de introdusert et nytt konsept, som var å kjøre lydopptak av rollespill. Det var blandet tilbakemeldinger fra studentene deres på den tiden. Noen var ukomfortable med det, noen var ukomfortable, men så nytten i å kunne høre tilbake igjen på egen stemme, og noen syntes det var helt greit. Problemet var at det var veldig mye arbeid som gikk inn i den oppgaven. Det å ta lydopptak og prosessere lydklippet for å sende det inn sammen med læringsnotat tok ganske lang tid, og de ønsket å kunne gi studentene mer effektiv, mer hyppig trening. Her kom jeg med forslaget om å benytte oss av kunstig intelligens for å få den samtalepartneren de da kunne snakke med når de trengte. I utgangspunktet skulle det være et litt større prosjekt med et ansikt vi faktisk snakket med, men det fikk vi ikke tid til, og vi utviklet hovedsakelig samtale-AI. Etter det gikk mye av tiden til å planlegge og kartlegge problemstilling og forskningsspørsmål, og til slutt var det mer fokus på å faktisk få ferdig enn AI. Det jobbet altså mye med å finne teknologien til å gjøre dette her. Teknologistekken vi landet på var en React-applikasjon med et API på serveren som snakket med lokalkjørt large language model. Denne llm-en jobbet som lærlingen, den samtalepartneren de skulle snakke med, mens large language modelen kjørte som det. For å få tekst til stemme og stemme til tekst benyttet jeg meg av OpenAI sitt API. Da er det et API-kall på serveren som sender ut et eksternt API-kall til Text-to-Speech og Speech-to-Text modellen. Dette tar jo da å enten transkriberer det du har sagt inn, eller lager en lydfil av det AI har sendt tilbake igjen basert på svaret fra large language model. Dette tar da å vises i webappen med både teksten og å spille av lydklipper automatisk. I applikasjonen er det muligheter for å klikke for å starte å snakke, og så stopper du å snakke når du er ferdig med å snakke. Og så venter du på svar fra AI. Når du får et svar tilbake igjen, så er det opp til deg å starte setningen din på nytt igjen. Noe som da gir deg muligheten til å tenke over et svar du ønsker å gi, og lese over det et par ganger til, hva er det du egentlig sa. I tillegg så er det en chat-funksjon. Hvis du ikke har tid eller mulighet til å snakke med AI og høre lyd tilbake igjen, så kan du sitte og skrive for å få samme treninger, bare i tekstformat. Og hvis du sitter for eksempel på et bibliotek eller noe i den delen, så er det utviklet det også. Det er akkurat det samme som i samtalen, bare uten text-to-speech og speech-to-text. Dette blir da samlet som en chat-logg. Den chat-loggen vil da bli vist på skjermen når du avslutter. Originalt så skulle dette komme sammen med en AI-generert tilbakemelding, i et veldig rigid JSON-format. Problemet landet på at du ikke kunne bruke det formatet på samme måte. Eller hver gang, når det var lengre samtaler, så kom den ut med et usikkert format tilbake igjen. Det er noe som ødela pdf-genereringen. Det er et pdf-bibliotek som genererer sånn at du kan laste ned rapporten ettertid og ta vare på den. Sånn sett er det ingenting som lagres noen sted. Hele chat-loggen kjøres på lokale maskiner, men den lagres ikke noen sted. Heller ikke stemmene eller lydfilene lagres noen sted. De forsvinner med en gang du refresher nettsiden. Utenom det så skulle det egentlig også være mulig å få lydfilen tilbake igjen, men det var det ikke tid til å lage. Det dykket opp problemer noen ganger under testing hvor genereringen av rapport ikke fungerte, eller tok veldig lang tid. Men det fungerte på ca. 3-4 av 7 deltakere i remote testing. Dette var usikkert hvorfor. Det fant vi ut av var på grunn av feedback-modulen. Dette var også the case uka de fikk mulighet for å teste dette hjemme, hvor jeg fant ut på slutten at det var flere som hadde nøyaktig samme problemer. Fikk klart å få gjenskapt problemet og fant ut av at dette faktisk var problemet med feedback-modulen. For å få dette klart i tide, så fjernet jeg bare feedback-modulen, og de fikk hovedsakelig bare en chat-log tilbake. Da kunne flere avstudentene faktisk gjennomføre testene, eller gjennomføre samtalene, og de kunne da svare på oppgaven sin. Når de snakker om dette i ettertid, så var det flere ganger dette skjedde. Det ble ikke rapportert til meg, men det har jo skjedd flere ganger. Det var en midlertidig løsning å fjerne det på, da jeg ville ha det opp å gå ganske fort, men jeg har nå i ettertid utviklet en ny feedback-funksjon, hvor de da har muligheten til å få generert tilbakemelding denne gangen med OpenAI sitt Completions API for å få mer kvalitet i tilbakemeldingen, da det er viktigere at det er godt norskt språk i tilbakemeldingen enn det er i selve samtalen. Målet i samtalen er å få mengdetrenere å få snakka, forstå hva de sier, forstå hva de kommer tilbake med, og få kasta ut andre løsninger, ikke nødvendigvis inneholde i hva de sier, i like stor grad som på tilbakemeldingen. På tilbakemeldingen er det litt viktig at det er ordentlig språk. Grunnet fysiske problemer med hardware som var tilgjengelig på både utviklingsmaskinen og på serverne jeg fikk fra UiA, så måtte jeg bruke lavere modeller av OLAMA, som er den lokale large language modellen. Jeg brukte modeller som var ikke egnet for norsk, men de fungerte på norsk. Det finnes bedre norske modeller, men de ble ikke brukt. Dette krevde annen teknologi og passet ikke inn i den eksisterende strukturen jeg hadde. Men det hadde ført til litt bedre svar, eller litt bedre norsk formulering. Problemet lamnet i at det var ofte direkte oversatt engelsk som kom inn, så det ble formulert rart i svaret. Men det er noe man kan se borti fra når man sitter og holder på. Det kan ha ført til litt mindre immersion, men anten det så burde det være greit. I utgangspunktet ønsket jeg å kjøre så lite OpenAI som mulig, da dette er noe som koster penger, og det er derfor jeg bruker det på mindre krevende operasjoner som text-to-speech, speech-to-text og rapportgenerering. Det krever mye mindre av selve brukeren, eller mindre av selve AI-en, enn det en hel samtale frem og tilbake og testing av samtalen og chat-completion ville gjort med selve OLAMA, og da får det mindre. I tillegg er fagskolen en offentlig etat under fylkeskommunen, og de skal derfor unngå å bruke OpenAI så godt det lar seg gjøre. Man kunne ha funnet og satt opp andre modeller for text-to-speech, speech-to-text og rapportgenerering, da dette kan gjøres med bedre utstyr. Jeg har et 15 år gammelt grafikkort som er hovedkilden til AI-en på serveren etter OEA. Med et nyere grafikkort, litt kraftigere, kunne man ha kjørt større og sterkere modeller lokalt for å få lignende resultater som OpenAI har.
